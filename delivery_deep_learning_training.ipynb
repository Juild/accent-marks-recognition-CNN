{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL #The Python Imaging Library (PIL) adds image processing capabilities to your Python interpreter\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation function with model creation and data processing inside. Data processing is inside the network. Data augmentation layers do not activate at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def crossvalidation(data_dir, img_size, batch_size, folds, epochs):\n",
    "    val_acc_list = []\n",
    "    train_acc_list = []\n",
    "    dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=(img_size[0],img_size[1]),\n",
    "        batch_size=batch_size)\n",
    "    num_classes = len(dataset.class_names)\n",
    "    print(dataset.class_names)\n",
    "    ds_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    print(ds_size)\n",
    "    dataset = dataset.shuffle(buffer_size=ds_size)\n",
    "    for fold in range(folds):\n",
    "        model = Sequential([\n",
    "            layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.1,0.1),\n",
    "                                                                  width_factor=(-0.1,0.1),\n",
    "                                                                  fill_mode=\"reflect\",\n",
    "                                                                 input_shape=img_size),\n",
    "            layers.experimental.preprocessing.RandomRotation(factor=(-0.05,0.05), fill_mode=\"reflect\"),\n",
    "            layers.experimental.preprocessing.Rescaling(1./255),\n",
    "            layers.Lambda(lambda x: tf.image.rgb_to_grayscale(x)),\n",
    "            layers.Lambda(lambda x: tf.image.adjust_contrast(x, 10)),\n",
    "            layers.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(num_classes)\n",
    "            ])\n",
    "        model.summary()\n",
    "        print(f\"########## FOLD {fold + 1} ##########\")\n",
    "        train_ds = dataset.shard(folds, index=fold)\n",
    "        for i in range(folds - 2):\n",
    "            train_ds = train_ds.concatenate(dataset.shard(folds, index=np.mod(fold+i+1,folds)))\n",
    "            print(\"concatenated\")\n",
    "        val_ds = dataset.shard(folds, index=np.mod(fold+folds-1, folds))\n",
    "        print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "        print(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "                  train_ds,\n",
    "                  validation_data=val_ds,\n",
    "                  epochs=epochs\n",
    "        )\n",
    "        val_acc_list.append(history.history[\"val_accuracy\"])\n",
    "        train_acc_list.append(history.history[\"accuracy\"])\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "    return train_acc_list, val_acc_list \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (128, 128, 3)\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "datadir =\"images_train_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to generate diferent k folds. range(1) means we only do 1 kfold. This was implemented as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for i in range(1):\n",
    "    train_accs, val_accs = crossvalidation(data_dir=datadir,\n",
    "                        img_size=img_size,\n",
    "                        batch_size=batch_size,\n",
    "                        folds=5,\n",
    "                        epochs=epochs)\n",
    "    print(f\"############### TRIAL {i + 1} ##############\")\n",
    "    train_acc_hist.append(train_accs)\n",
    "    val_acc_hist.append(val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations to obtain the mean accuracy for training and validation between the diferent folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_acc_hist = []\n",
    "std_train_acc_hist = []\n",
    "mean_val_acc_hist = []\n",
    "std_val_acc_hist = []\n",
    "for trial in train_acc_hist:\n",
    "    for i in range(len(trial[0])):\n",
    "        avg_acc_epoch = []\n",
    "        for fold in trial:\n",
    "            avg_acc_epoch.append(fold[i])\n",
    "        mean_train_acc_hist.append(np.mean(avg_acc_epoch))\n",
    "        std_train_acc_hist.append(np.std(avg_acc_epoch))\n",
    "        \n",
    "\n",
    "# Do the same for validation accuracy\n",
    "for trial in val_acc_hist:\n",
    "    for i in range(len(trial[0])):\n",
    "        avg_acc_epoch = []\n",
    "        for fold in trial:\n",
    "            avg_acc_epoch.append(fold[i])\n",
    "        mean_val_acc_hist.append(np.mean(avg_acc_epoch))\n",
    "        std_val_acc_hist.append(np.std(avg_acc_epoch))\n",
    "print(f\"Train mean final acc: {np.mean(mean_train_acc_hist[-4:])}with std {np.std(mean_train_acc_hist[-4:])}\")\n",
    "print(f\"Validation mean final acc: {np.mean(mean_val_acc_hist[-4:])} with std {np.std(mean_val_acc_hist[-4:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.errorbar(np.arange(1, epochs + 1),\n",
    "            mean_train_acc_hist,\n",
    "            label=\"Mean Trainning set accuracy\",\n",
    "            yerr = std_train_acc_hist)\n",
    "ax.errorbar(np.arange(1, epochs + 1),\n",
    "        mean_val_acc_hist,\n",
    "        label=\"Mean Validation set accuracy\",\n",
    "        yerr = std_val_acc_hist)\n",
    "plt.xticks(range(epochs+1))\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "fig.savefig(\"foo.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train definitive model with the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=(img_size[0], img_size[1]),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "model = Sequential([\n",
    "            layers.experimental.preprocessing.RandomTranslation(height_factor=(-0.1,0.1),\n",
    "                                                                  width_factor=(-0.1,0.1),\n",
    "                                                                  fill_mode=\"reflect\",\n",
    "                                                                 input_shape=img_size),\n",
    "            layers.experimental.preprocessing.RandomRotation(factor=(-0.05,0.05), fill_mode=\"reflect\"),\n",
    "            layers.experimental.preprocessing.Rescaling(1./255),\n",
    "            layers.Lambda(lambda x: tf.image.rgb_to_grayscale(x)),\n",
    "            layers.Lambda(lambda x: tf.image.adjust_contrast(x, 10)),\n",
    "            layers.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "          dataset,\n",
    "          epochs=epochs\n",
    ")\n",
    "#serialize to JSON\n",
    "model_json=model.to_json()\n",
    "with open(\"model.json\",\"w\") as json_file:\n",
    "  json_file.write(model_json)\n",
    "#serialize weights to hdf5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"saved model to disk\")\n",
    "model.save('modeltotal.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}